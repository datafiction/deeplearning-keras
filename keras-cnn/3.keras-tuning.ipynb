{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - Tuning Neural Network (Advanced)\n",
    "\n",
    "Tuning: Regularization, learning rate, Droupout, Maxpooling etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regularization: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1  Base lIne model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 65.03% (8.60%)\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model on the Sonar Dataset\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = read_csv(\"./data/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "\t# Compile model\n",
    "\tsgd = SGD(lr=0.01,\n",
    "              momentum=0.8,\n",
    "              decay=0.0,\n",
    "              nesterov=False)\n",
    "    \n",
    "    \n",
    "\tmodel.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "\treturn model\n",
    "\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp',\n",
    "                   KerasClassifier(build_fn=create_baseline,\n",
    "                    epochs=10,\n",
    "                    batch_size=16,\n",
    "                    verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. Dropout at visible layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible: 81.66% (7.78%)\n"
     ]
    }
   ],
   "source": [
    "# Example of Dropout on the Sonar Dataset: Visible Layer\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = read_csv(\"./data/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "\n",
    "\n",
    "# dropout in the input layer with weight constraint\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dropout(0.2, input_shape=(60,)))\n",
    "\tmodel.add(Dense(60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "\t# Compile model\n",
    "    \n",
    "\tsgd = SGD(lr=0.1,\n",
    "              momentum=0.9,\n",
    "              decay=0.0,\n",
    "              nesterov=False)\n",
    "    \n",
    "\tmodel.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "\n",
    "estimators.append(('mlp',\n",
    "                   KerasClassifier(build_fn=create_model,\n",
    "                                   epochs=10,\n",
    "                                   batch_size=16,\n",
    "                                   verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "\n",
    "print(\"Visible: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3. Using Dropout on Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden: 82.21% (6.82%)\n"
     ]
    }
   ],
   "source": [
    "# Example of Dropout on the Sonar Dataset: Hidden Layer\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = read_csv(\"./data/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "\n",
    "\n",
    "# dropout in hidden layers with weight constraint\n",
    "def create_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "\t# Compile model\n",
    "\tsgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "\treturn model\n",
    "\n",
    "\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "\n",
    "estimators.append(('mlp',\n",
    "                   KerasClassifier(build_fn=create_model,\n",
    "                                   epochs=10,\n",
    "                                   batch_size=16,\n",
    "                                   verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "\n",
    "print(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 Time-Based Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 235 samples, validate on 116 samples\n",
      "Epoch 1/10\n",
      "0s - loss: 0.6814 - acc: 0.6468 - val_loss: 0.6388 - val_acc: 0.8707\n",
      "Epoch 2/10\n",
      "0s - loss: 0.6391 - acc: 0.7319 - val_loss: 0.5352 - val_acc: 0.8276\n",
      "Epoch 3/10\n",
      "0s - loss: 0.5676 - acc: 0.8128 - val_loss: 0.4886 - val_acc: 0.8103\n",
      "Epoch 4/10\n",
      "0s - loss: 0.4886 - acc: 0.8298 - val_loss: 0.4386 - val_acc: 0.9397\n",
      "Epoch 5/10\n",
      "0s - loss: 0.4141 - acc: 0.8511 - val_loss: 0.3349 - val_acc: 0.9483\n",
      "Epoch 6/10\n",
      "0s - loss: 0.3567 - acc: 0.8766 - val_loss: 0.4452 - val_acc: 0.8793\n",
      "Epoch 7/10\n",
      "0s - loss: 0.3217 - acc: 0.9021 - val_loss: 0.2296 - val_acc: 0.9655\n",
      "Epoch 8/10\n",
      "0s - loss: 0.2932 - acc: 0.8936 - val_loss: 0.2142 - val_acc: 0.9655\n",
      "Epoch 9/10\n",
      "0s - loss: 0.2658 - acc: 0.9021 - val_loss: 0.2515 - val_acc: 0.9569\n",
      "Epoch 10/10\n",
      "0s - loss: 0.2366 - acc: 0.9234 - val_loss: 0.2278 - val_acc: 0.9569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x133eb9358>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Based Learning Rate Decay\n",
    "from pandas import read_csv\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(\"./data/ionosphere.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Compile model\n",
    "epochs = 10\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "decay_rate = learning_rate / epochs\n",
    "\n",
    "momentum = 0.8\n",
    "\n",
    "sgd = SGD(lr=learning_rate,\n",
    "          momentum=momentum,\n",
    "          decay=decay_rate,\n",
    "          nesterov=False)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y,\n",
    "          validation_split=0.33,\n",
    "          epochs=epochs,\n",
    "          batch_size=28,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Drop-Based Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 235 samples, validate on 116 samples\n",
      "Epoch 1/10\n",
      "0s - loss: 0.6803 - acc: 0.6468 - val_loss: 0.6199 - val_acc: 0.9138\n",
      "Epoch 2/10\n",
      "0s - loss: 0.6195 - acc: 0.7234 - val_loss: 0.4761 - val_acc: 0.8621\n",
      "Epoch 3/10\n",
      "0s - loss: 0.4985 - acc: 0.8255 - val_loss: 0.3681 - val_acc: 0.9483\n",
      "Epoch 4/10\n",
      "0s - loss: 0.3626 - acc: 0.8596 - val_loss: 0.3826 - val_acc: 0.8707\n",
      "Epoch 5/10\n",
      "0s - loss: 0.2804 - acc: 0.8809 - val_loss: 0.1543 - val_acc: 0.9655\n",
      "Epoch 6/10\n",
      "0s - loss: 0.2153 - acc: 0.9191 - val_loss: 0.2157 - val_acc: 0.9310\n",
      "Epoch 7/10\n",
      "0s - loss: 0.1795 - acc: 0.9362 - val_loss: 0.1501 - val_acc: 0.9655\n",
      "Epoch 8/10\n",
      "0s - loss: 0.1584 - acc: 0.9319 - val_loss: 0.0850 - val_acc: 0.9741\n",
      "Epoch 9/10\n",
      "0s - loss: 0.1722 - acc: 0.9404 - val_loss: 0.1417 - val_acc: 0.9741\n",
      "Epoch 10/10\n",
      "0s - loss: 0.1257 - acc: 0.9617 - val_loss: 0.1020 - val_acc: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f115710>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop-Based Learning Rate Decay\n",
    "import pandas\n",
    "from pandas import read_csv\n",
    "import numpy\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "  initial_lrate = 0.1\n",
    "  drop = 0.5\n",
    "  epochs_drop = 10.0\n",
    "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "  return lrate\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(\"./data/ionosphere.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim=34, kernel_initializer= 'normal' , activation= 'relu' ))\n",
    "model.add(Dense(1, kernel_initializer= 'normal' , activation= 'sigmoid' ))\n",
    "\n",
    "\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer=sgd, metrics=[ 'accuracy' ])\n",
    "\n",
    "\n",
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y,\n",
    "          validation_split=0.33,\n",
    "          epochs=10,\n",
    "          batch_size=28,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
